{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a2285c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ed3164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = VOCSegmentation(root='data/', year='2012', image_set='train', download=True)\n",
    "\n",
    "# writing custom dataset, inheriting from VOCSegmentation dataset\n",
    "class VOCSegmentationWithPIL(VOCSegmentation):\n",
    "    def __init__(self, root='data', year='2012', image_set='train',\n",
    "                 download=True, image_size=(224, 224)):\n",
    "        super().__init__(root=root, year=year, image_set=image_set, download=download)\n",
    "        self.image_resize = T.Resize(image_size)\n",
    "        self.mask_transform = T.Compose([\n",
    "            T.Resize(image_size, interpolation=Image.NEAREST),\n",
    "            T.PILToTensor(),  # Keeps label values intact\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, mask = super().__getitem__(index)\n",
    "        image = self.image_resize(image)  # still PIL.Image\n",
    "        mask = self.mask_transform(mask).squeeze(0).long()  # [H, W] as LongTensor\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aace71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_pil(batch):\n",
    "    images, masks = zip(*batch)  # tuple of lists\n",
    "    return list(images), torch.stack(masks)  # keep images as list of PIL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66498e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class KITTISegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, image_size=(224, 224)):\n",
    "        self.image_paths = sorted([os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith('.png') or fname.endswith('.jpg')])\n",
    "        self.mask_paths = sorted([os.path.join(mask_dir, fname) for fname in os.listdir(mask_dir) if fname.endswith('.png')])\n",
    "\n",
    "        self.image_transform = T.Compose([\n",
    "            T.Resize(image_size),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "        ])\n",
    "        \n",
    "        self.mask_transform = T.Compose([\n",
    "            T.Resize(image_size, interpolation=Image.NEAREST),\n",
    "            T.PILToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        mask = Image.open(self.mask_paths[idx])  # Assumes masks are already in correct class format\n",
    "        return self.image_transform(image), self.mask_transform(mask).squeeze(0).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c29e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_dataset = KITTISegmentationDataset(\n",
    "    image_dir='kitti_data/training/image_2',\n",
    "    mask_dir='kitti_data/training/semantic',\n",
    "    image_size=(224, 224)\n",
    ")\n",
    "\n",
    "\n",
    "kitti_loader = DataLoader(\n",
    "    kitti_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ad892de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for images, masks in kitti_loader:\n",
    "        # print(images.shape, masks.shape)\n",
    "        # print(\"Unique labels in masks:\", torch.unique(masks))\n",
    "        count += 1 \n",
    "        # if count ==100:\n",
    "        #         break\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed00650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = DINO_Mask2Former_Segmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a9a4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"Trainable: {name}\")\n",
    "#     else:\n",
    "#         print(f\"Frozen: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0208f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f431d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_losses, val_ious, train_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a94f3b",
   "metadata": {},
   "source": [
    "### Evaluating on KITTI Dataset without Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c19fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DinoSegModel(freeze_dino=True, num_classes=21).to(device)\n",
    "# model.load_state_dict(torch.load(\"/home/iiitb/Desktop/anant/playground/ProjectBytes/best_model1.pth\", map_location=device))\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c7784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mIoU:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mIoU: 100%|██████████| 25/25 [00:02<00:00,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean IoU over validation set: 0.0005\n",
      "KITTI mIoU: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import MulticlassJaccardIndex\n",
    "\n",
    "NUM_CLASSES = 34   # 0 to 33 possible\n",
    "IGNORE_INDEX = 255\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize mIoU metric\n",
    "# miou_metric = MulticlassJaccardIndex(\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     ignore_index=IGNORE_INDEX,\n",
    "# ).to(device)\n",
    "\n",
    "miou_metric = MulticlassJaccardIndex(num_classes=34, ignore_index=255).to(device)\n",
    "\n",
    "def evaluate_kitti(model, loader, device):\n",
    "    model.eval()\n",
    "    miou_metric.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loop = tqdm(loader, desc=\"Evaluating mIoU\")\n",
    "\n",
    "        for step, (images, masks) in enumerate(val_loop):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            images = T.Normalize(mean=[0.5]*3, std=[0.5]*3)(images)\n",
    "\n",
    "            outputs, _, _ = model(images) \n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            miou_metric.update(preds, masks)\n",
    "\n",
    "    mean_iou = miou_metric.compute().item()\n",
    "    print(f\"\\nMean IoU over validation set: {mean_iou:.4f}\")\n",
    "    return mean_iou\n",
    "\n",
    "val_miou = evaluate_kitti(model, kitti_loader, device)\n",
    "print(f\"KITTI mIoU: {val_miou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c46cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
